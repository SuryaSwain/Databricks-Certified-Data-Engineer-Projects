# Project 4: Using Spark SQL for Analysis

## Overview

This project focuses on analyzing transformed data using Spark SQL. The objective is to leverage Spark SQL capabilities to extract meaningful insights from a dataset, enabling effective data analysis and reporting.

## Objectives

- Use Spark SQL to query and analyze transformed sales data.
- Extract key metrics such as total sales, average order value, and other relevant statistics.
- Document SQL queries and their corresponding results for better understanding.

## Available Data Source

- Use the transformed data generated from **Project 3: ETL Process with Spark**.

## Files in This Project

- **spark_sql_analysis.ipynb**: Jupyter Notebook containing the Spark SQL queries and analysis results.
- **query_results.csv**: A CSV file (if applicable) to store the output of SQL queries for reference.
- **requirements.txt**: List of required Python packages to run the project.

## Getting Started

To run this project, follow these steps:

1. Clone the repository or download the project files.
2. Install the required packages using the following command:
   ```bash
   pip install -r requirements.txt
   ```
3. Open the Jupyter Notebook:
   ```bash
   jupyter notebook spark_sql_analysis.ipynb
   ```
4. Follow the instructions in the notebook to execute the Spark SQL queries and analyze the data.

## Reference Materials

- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Using SQL with Databricks](https://docs.databricks.com/sql/index.html)

## Outcome

By the end of this project, you will have hands-on experience in using Spark SQL for data analysis. You will be able to write SQL queries to extract insights from the transformed sales data and understand how to leverage SQL within the Spark environment.

## License

This project is licensed under the MIT License. See the [LICENSE](../../LICENSE) file for details.
